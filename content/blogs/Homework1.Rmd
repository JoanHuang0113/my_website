---
categories:  
- ""    #the front matter should be like the one found in, e.g., blog2.md. It cannot be like the normal Rmd we used
- ""
date: "2021-09-30"
description: Homework1 # the title that will show up once someone gets to this page
draft: false
image: spices.jpg # save picture in \static\img\blogs. Acceptable formats= jpg, jpeg, or png . Your iPhone pics wont work

keywords: ""
slug: Homework1 # slug is the shorthand URL address... no spaces plz
title: Homework1

output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---
```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```

```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(fivethirtyeight)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(tidyquant)
library(rvest) # to scrape wikipedia page
```

# Where Do People Drink The Most Beer, Wine And Spirits?

Back in 2014, [fivethiryeight.com](https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/) published an article on alchohol consumption in different countries. 

```{r, load_alcohol_data}
library(fivethirtyeight)
data(drinks)


# or download directly
# alcohol_direct <- read_csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/alcohol-consumption/drinks.csv")

```

What are the variable types? Any missing values we should worry about?

There is one qualitative variable (country), and the rest of the variables are numerical (beer, wine, and spirit servings, as well as total litres of pure alcohol consumed). It does not appear that there are any missing values that we need to worry about.

```{r glimpse_skim_data}
glimpse(drinks)

```

```{r beer_plot}
#Create Graph
drinks %>% 
  slice_max(order_by = beer_servings, n=25) %>% #Order for top 25 countries by beer consumption
  ggplot(aes(x = beer_servings, y = fct_reorder(country, beer_servings))) +
  geom_col() +
  labs(title = "Top 25 Beer Consuming Countries", x = "Servings of Beer (cans)", y = "Country")


```

```{r wine_plot}
#Create Graph
drinks %>% 
  slice_max(order_by = wine_servings, n=25) %>% #Order for top 25 countries by wine consumption
  ggplot(aes(x = wine_servings, y = fct_reorder(country, wine_servings))) +
  geom_col() + 
  labs(title = "Top 25 Wine Consuming Countries", x = "Servings of Wine (glasses)", y = "Country")


```

```{r spirit_plot}
#Create Graph
drinks %>% 
  slice_max(order_by = spirit_servings, n=25) %>% #Order for top 25 countries by spririt consumption
  ggplot(aes(x = spirit_servings, y = fct_reorder(country, spirit_servings), fill = country)) +
  geom_col() +
  theme(legend.position = "None") +
  labs(title = "Top 25 Spirit Consuming Countries", x = "Servings of Spirits (shots)", y = "Country")

```

What can you infer from these plots? 

> TYPE YOUR ANSWER AFTER (AND OUTSIDE!) THIS BLOCKQUOTE.

Surprisingly, the top beer drinking country was Namibia. After a little bit of research, we found out that Namibia used to be a German colony, so it is possible that the habits and drinking culture of Germany was passed on to the people of Namibia. Countries like the Czech Republic, Germany, Poland, and Lithuania are all in the same general area of Eastern Europe. This may indicate that the drinking culture in this area leans heavily towards beer. For wine, the countries that are in the top 25 was not too surprising. France and Portugal are large wine-producing countries, so wine there is high quality for lesser prices than in other areas of the world and is highly accessible. Even countries on the lower end of the scale still drink a significant amount of wine, possibly due to low import costs and an overall lower price per serving.

The most surprising graph was for spirit consumption. Some of the top countries, like Grenada and St. Lucia, are Caribbean countries, so they probably have very relaxed ways of life and alcohol may play a big part in this. Furthermore, many Caribbean countries produce rum, hence making it very accessible to the people who live there (when compared to imported drinks). Additionally, the Russian Federation and Belarus produce lots of vodka and enjoy drinking it, so it was no surprise to see them in the top 25.

# Analysis of Movies- IMDB dataset

We will look at a subset sample of movies, taken from the [Kaggle IMDB 5000 movie dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset).

```{r,load_movies, warning=FALSE, message=FALSE}

movies <- read_csv(here::here("data", "movies.csv"))
glimpse(movies)

```

## Use your data import, inspection, and cleaning skills to answer the following:

-   Are there any missing values (NAs)? Are all entries distinct or are there duplicate entries?

While it does not appear that there were any missing values in the dataset, it does appear as though there were duplicate entries. This is because in the original dataset above,there were 2,961 rows of data. After removing potential duplicates by the distinct identifier "Title", there are now only 2,907 rows of data, suggesting that duplicate movie titles had been within the data. When further observing the raw data, the movie titles that were duplicated only differed in the "votes" category. This may be because the data was updated at a later date, and the original entry was not removed.
```{r data_cleaning}
#Search for missing values (NAs)
sum(is.na(movies)) #Does not appear there are any missing values

#Search for duplicate entries based on title
sum(duplicated(movies$title))

#Remove duplicate rows based on Title (distinct identifier)
movies %>% 
  distinct(title, .keep_all = TRUE)

```


```{r, table_moviegenre}
#Create table of movie genres
movies %>%
    group_by(genre) %>% #group by genre
    count(sort=TRUE) #rank by descending order

```

```{r, table_moviereturns}
#Create table with average gross earnings and budget
movies %>%
  group_by(genre) %>% #sort by genre
  summarize(average_gross = mean(gross), 
         average_budget = mean(budget),
         return_on_budget = (average_gross/average_budget) -1) %>% #create avg. variables, and return on budget variable
  arrange(desc(return_on_budget)) #rank in descending order

```

```{r, directors_highestgross}
#Create table showing top 15 directors by gross revenue
movies %>%
  group_by(director) %>% #sort by director
  summarize(average_gross = mean(gross), 
         median_gross = median(gross),
         std_dev_gross = sd(gross),
          total_gross = sum(gross)) %>% #create mean, median, standard deviation, and total gross revenue variables
  arrange(desc(total_gross)) %>% #arrange in descending order by total gross revenue
  head(directors_highestgross, n=15) #only include the top 15 directors based on total gross revenue
 


```

```{r, ratings_bygenre}
#Create table describing ratings by genre
movies %>%
  group_by(genre)%>% #sort by genre
  summarise(mean_rating = mean(rating), 
            min_rating = min(rating), 
            max_rating=max(rating), 
            median_rating = median(rating), 
            sd_rating = sd(rating)) %>%  #create summary statistics on ratings
  arrange(desc(mean_rating)) #arranged in descending order based on mean rating

#Create histogram
ggplot(movies, aes(x=rating, fill = genre)) +
  geom_histogram() +
  labs(title = "Ratings Distributed by Genre",
       x="Ratings",
       y="Count")

```


## Use `ggplot` to answer the following

-   Examine the relationship between `gross` and `cast_facebook_likes`. Produce a scatterplot and write one sentence discussing whether the number of facebook likes that the cast has received is likely to be a good predictor of how much money a movie will make at the box office. What variable are you going to map to the Y- and X- axes?

The number of facebook likes that a cast receives is not likely to be a good predictor of how much money a movie will make at the box office, as seen through the lack of correlation in the graph and the correlation test. Since we were testing if gross revenue is *dependent* on the number of facebook likes, gross revenue became the dependent variable (y-axis) and facebook likes became the independent variable (x-axis).

```{r, gross_on_fblikes}
#Create scatterplot
ggplot(movies, aes(x=cast_facebook_likes, y=gross)) + #assign x- and y-axis variables 
  geom_point() +
  geom_smooth(method="lm") +
  scale_y_log10()+
  scale_x_log10() + #scale axis to make graph readable
  labs(title = "Gross Revenue of Movie vs Cast Facebook Likes",
       x="Cast Facebook Likes",
       y="Gross Revenue") 

#Test correlation of variables
cor(x=movies$cast_facebook_likes, y=movies$gross)

```

-   Examine the relationship between `gross` and `budget`. Produce a scatterplot and write one sentence discussing whether budget is likely to be a good predictor of how much money a movie will make at the box office.

Through the scatterplot and correlation test we can assume that the budget can be a good predictor of how much money the movie will make. This makes sense since movies with bigger budgets tend to have more popular casts, better quality production, and more marketing campaigns.


```{r, gross_on_budget}
#Create scatterplot
ggplot(movies, aes(x=budget, y=gross)) + #assign x- and y-axis variables
  geom_point() +
  geom_smooth(method="lm") +
  scale_y_log10()+
  scale_x_log10() + #scale axis to make graph readable
  labs(title = "Gross Revenue of Movie vs Movie Budget",
       x="Movie Budget",
       y="Gross Revenue") 

#Test correlation of variables
cor(x=movies$budget, y=movies$gross)

```

-   Examine the relationship between `gross` and `rating`. Produce a scatterplot, faceted by `genre` and discuss whether IMDB ratings are likely to be a good predictor of how much money a movie will make at the box office. Is there anything strange in this dataset?

The relationship between IMDB Movie Rating and the amount of money a movie will make at the box office varies depending on genre, as seen in the scatterplots below. For some genres, like fantasy, horror, documentaries, and crime, there is seemingly no correlation of higher ratings receiving higher gross revenues. However, in some genres, like action and adventure, there is a slightly positive correlation with higher rated movies tending to attain higher amounts of success at the box office. For some genres (i.e., family and musical), there may be a positive correlation between the rating and gross profit, but there are not enough data points to come to a conclusive answer.

```{r, gross_on_rating}
#Create graph
ggplot(movies, aes(x=rating, y=gross)) + #Assign x- and y-axis variables
  geom_point() +
  geom_smooth(method="lm") +
  labs(title = "Gross Revenue of Movie vs IMDB Rating",
       x="IMDB Movie Rating",
       y="Gross Revenue") +
  facet_wrap(~genre) #group scatterplots by genre

```

# Returns of financial stocks

```{r load_nyse_data, message=FALSE, warning=FALSE}
nyse <- read_csv(here::here("data","nyse.csv"))
```

```{r companies_per_sector}
#Create table on companies per sector
stocks <- nyse %>%
  group_by(sector) %>% #Group data by sector
  summarize(count = n()) #Sort data in descending order

#Create bar plot
ggplot(stocks, aes(x=count, y=reorder(sector, count), fill=sector)) +
  geom_bar(stat="identity") +
  theme(legend.position="none") +
  labs(title="Company Count by Sector", 
       x="Number of Companies", y="Sector")


```

```{r, tickers_from_wikipedia}

djia_url <- "https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average"


#get tables that exist on URL
tables <- djia_url %>% 
  read_html() %>% 
  html_nodes(css="table")


# parse HTML tables into a dataframe called djia. 
# Use purr::map() to create a list of all tables in URL
djia <- map(tables, . %>% 
               html_table(fill=TRUE)%>% 
               clean_names())


# constituents
table1 <- djia[[2]] %>% # the second table on the page contains the ticker symbols
  mutate(date_added = ymd(date_added),
         
         # if a stock is listed on NYSE, its symbol is, e.g., NYSE: MMM
         # We will get prices from yahoo finance which requires just the ticker
         
         # if symbol contains "NYSE*", the * being a wildcard
         # then we jsut drop the first 6 characters in that string
         ticker = ifelse(str_detect(symbol, "NYSE*"),
                          str_sub(symbol,7,11),
                          symbol)
         )

# we need a vector of strings with just the 30 tickers + SPY
tickers <- table1 %>% 
  select(ticker) %>% 
  pull() %>% # pull() gets them as a sting of characters
  c("SPY") # and lets us add SPY, the SP500 ETF

```

Now let us download prices for all 30 DJIA constituents and the SPY ETF that tracks SP500 since January 1, 2020

```{r get_price_data, message=FALSE, warning=FALSE, cache=TRUE}
# Notice the cache=TRUE argument in the chunk options. Because getting data is time consuming, # cache=TRUE means that once it downloads data, the chunk will not run again next time you knit your Rmd

myStocks <- tickers %>% 
  tq_get(get  = "stock.prices",
         from = "2000-01-01",
         to   = Sys.Date()) %>% # Sys.Date() returns today's price
  group_by(symbol) 

glimpse(myStocks) # examine the structure of the resulting data frame
```

Financial performance analysis depend on returns; If I buy a stock today for 100 and I sell it tomorrow for 101.75, my one-day return, assuming no transaction costs, is 1.75%. So given the adjusted closing prices, our first step is to calculate daily and monthly returns.

```{r calculate_returns, message=FALSE, warning=FALSE, cache=TRUE}
#calculate daily returns
myStocks_returns_daily <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily", 
               type       = "log",
               col_rename = "daily_returns",
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               type       = "arithmetic",
               col_rename = "monthly_returns",
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual <- myStocks %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "yearly", 
               type       = "arithmetic",
               col_rename = "yearly_returns",
               cols = c(nested.col))
```

```{r summarise_monthly_returns}
#Create table summarizing each stock and 'SPY'
new_table = myStocks_returns_monthly %>%
  summarise(returns_mean = mean(monthly_returns),
            returns_median = median(monthly_returns),
            returns_min = min(monthly_returns),
             returns_max = max(monthly_returns),
            returns_SD = sd(monthly_returns)) #Create columns to display summary statistics of stocks and SPY
print(new_table)

 

```

```{r density_monthly_returns}
#Create density plot for each stock
myStocks_returns_monthly %>%
  ggplot(aes(x=monthly_returns))+ facet_wrap(vars(symbol)) + 
  labs(x= "Monthly Returns", y="Frequency of Return") +
  geom_density()

```

What can you infer from this plot? Which stock is the riskiest? The least risky?

From the various density plots, it is discernible that those stocks with a wider density plot are more risky. Owing to which, in our opinion, Apple and CRM were the riskiest stocks in the observed time frame. One the other hand, SPY seems to be the least risky. This is likely because SPY is an ETF and therefore aggregates risk, unlike individual stocks.


```{r risk_return_plot}
#Create plot showing expected monthly returns and risk of the stocks
new_table %>%
  ggplot(aes(x= returns_SD, y= returns_mean))+ 
  geom_point() + 
  ggrepel::geom_text_repel(aes(label = symbol)) + #Label stocks with tickers
  labs(
    title = "Relationship between SD and Expected Monthly Return",
    x = 'Standard Deviation of Monthly Returns',
    y = 'Expected monthly Return'
  ) +
  NULL


```

What can you infer from this plot? Are there any stocks which, while being riskier, do not have a higher expected return?

It can be inferred that stocks with a higher risk also have a higher expected average monthly return. This follows standard economic theory - the higher the risk, the higher the return. Two stocks that stood out a bit for being relatively risky but without a corresponding higher expected return are DOW and CSCO. Both of the these names had relatively higher standard deviations, but they appeared to fall below the typical expected returns when compared to stocks with similar risk profiles. 

# Is inflation transitory?

A recent study by the Bank for International Settlements (BIS) claimed that the [Current Inflation Spike Is Just Transitory](https://www.bloomberg.com/news/articles/2021-09-20/current-inflation-spike-is-just-transitory-new-bis-study-argues). As the article says,

> The surge in inflation seen across major economies is probably short lived because it's confined to just a few sectors of the economy, according to the Bank for International Settlements.

> New research by the BIS's Claudio Borio, Piti Disyatat, Egon Zakrajsek and Dora Xia adds to one of the hottest debates in economics -- how long the current surge in consumer prices will last. Both Federal Reserve Chair Jerome Powell and his euro-area counterpart Christine Lagarde have said the pickup is probably transitory, despite a snarled global supply chain and a spike in energy prices.

You have to download data for CPI and the 10 year bill and produce the following graph

```{r cpi_10year, echo=FALSE, out.width="90%"}
knitr::include_graphics(here::here("images", "cpi_10year.png"), error = FALSE)
```

```{r, get_cpi_10Year_yield}

cpi  <-   tq_get("CPIAUCSL", get = "economic.data",
                       from = "1980-01-01") %>% 
  rename(cpi = symbol,  # FRED data is given as 'symbol' and 'price'
         rate = price) %>% # we rename them to what they really are, e.g., cpi and rate
  
  # calculate yearly change in CPI by dividing current month by same month a year (or 12 months) earlier, minus 1
  mutate(cpi_yoy_change = rate/lag(rate, 12) - 1)

ten_year_monthly  <-   tq_get("GS10", get = "economic.data",
                       from = "1980-01-01") %>% 
  rename(ten_year = symbol,
         yield = price) %>% 
  mutate(yield = yield / 100) # original data is not given as, e.g., 0.05, but rather 5, for five percent

# we have the two dataframes-- we now need to join them, and we will use left_join()
# base R has a function merge() that does the same, but it's slow, so please don't use it

mydata <- 
  cpi %>% 
  left_join(ten_year_monthly, by="date") %>% 
  mutate(
    year = year(date), # using lubridate::year() to generate a new column with just the year
    month = month(date, label = TRUE),
    decade=case_when(
      year %in% 1980:1989 ~ "1980s",
      year %in% 1990:1999 ~ "1990s",
      year %in% 2000:2009 ~ "2000s",
      year %in% 2010:2019 ~ "2010s",
      TRUE ~ "2020s"
      )
  )

#Create Graph
ggplot(mydata, aes(x=cpi_yoy_change, y=yield, colour = decade)) +
  geom_point() +
  geom_smooth(method=lm) +
  facet_grid(rows = vars(decade)) + #group by decade
  theme(legend.position = "none") + #remove legend
  labs(title = "What is the Relationship Between CPI and 10-yr Yield?", 
       x= "CPI Yearly Change", y="10 Year Treasury Constant Maturity Rate")

  

```

The graphs that we created and that are depicted above show the possible relationships between CPI and the 10 year Treasury yield. The graphs from the original source are all on the same scale and use different numbers on the axis, while our graphs utilize the same scale on both axis. This is allows us to better understand the relationships and trends occurring from the 1980s to the 2020s more clearly. For example, in the 1980s, the 10 year Treasury rate was much higher, and there was a corresponding higher CPI yearly change. Looking at the present day where the 10 year Treasury rate is extremely low, we can easily see the corresponding lower CPI yearly change through our graphs. In the 1980s, the spread of yearly CPI change was the greatest from 0.02 to 0.12. This is most likely due to the varying economic policies of presidents during the decade. Carter's administration saw very high levels of inflation and when Reagan came into office, he worked to try and implement policy hat would unwind this. Volcker, as chairman of the Fed, worked with the rest of the board to reduce inflation from its peak of 13.5% to almost 3% by 1983.


# Challenge 2: Opinion polls for the 2021 German elections

The Guardian newspaper has an [election poll tracker for the upcoming German election](https://www.theguardian.com/world/2021/aug/20/german-election-poll-tracker-who-will-be-the-next-chancellor).

The following code will scrape the wikipedia page and import the table in a dataframe.

```{r, scrape_wikipedia_polling_data, warnings= FALSE, message=FALSE}
url <- "https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election"

# similar graphs and analyses can be found at 
# https://www.theguardian.com/world/2021/jun/21/german-election-poll-tracker-who-will-be-the-next-chancellor
# https://www.economist.com/graphic-detail/who-will-succeed-angela-merkel


# get tables that exist on wikipedia page 
tables <- url %>% 
  read_html() %>% 
  html_nodes(css="table")


# parse HTML tables into a dataframe called polls 
# Use purr::map() to create a list of all tables in URL
polls <- map(tables, . %>% 
             html_table(fill=TRUE)%>% 
             janitor::clean_names())


# list of opinion polls
german_election_polls <- polls[[1]] %>% # the first table on the page contains the list of all opinions polls
  slice(2:(n()-1)) %>%  # drop the first row, as it contains again the variable names and last row that contains 2017 results
  mutate(
         # polls are shown to run from-to, e.g. 9-13 Aug 2021. We keep the last date, 13 Aug here, as the poll date
         # and we extract it by picking the last 11 characters from that field
         end_date = str_sub(fieldwork_date, -11),
         
         # end_date is still a string, so we convert it into a date object using lubridate::dmy()
         end_date = dmy(end_date),
         
         # we also get the month and week number from the date, if we want to do analysis by month- week, etc.
         month = month(end_date),
         week = isoweek(end_date)
         )

```

``` {r, moving_average_dataframes, warnings = FALSE, message = FALSE}
#Create a 14-day moving average for al parties
dataframe_election <- german_election_polls %>%
  select(union,spd,af_d,fdp,linke,grune,end_date) %>% #select data that must be manipulated
  mutate(
    union_RA =zoo::rollmean(union, k=14, fill= NA),
    spd_RA =zoo::rollmean(spd, k=14, fill= NA),
    af_d_RA =zoo::rollmean(af_d, k=14, fill= NA),
    fdp_RA =zoo::rollmean(fdp, k=14, fill= NA),
    linke_RA =zoo::rollmean(linke, k=14, fill= NA),
    grune_RA =zoo::rollmean(grune, k=14, fill= NA), #create rolling averages (means) for each political party
  ) %>%
  select(union_RA,spd_RA,af_d_RA,fdp_RA,linke_RA,grune_RA,end_date) #select out the specific data necessary to create graph

```

``` {r, individual_dataframes, warnings = FALSE, message = FALSE}
#Create individual dataframes for each political party
union_df <- dataframe_election %>% #create new dataframe
  select(union_RA, end_date) %>% #select party specific data
  rename(percentage_party = union_RA) #rename specific party percentage and repeat for all parties

spd_df <- dataframe_election %>%
  select(spd_RA, end_date) %>%
  rename(percentage_party = spd_RA)

af_d_df <- dataframe_election %>%
  select(af_d_RA, end_date) %>%
  rename(percentage_party = af_d_RA)

fdp_df <- dataframe_election %>%
  select(fdp_RA, end_date) %>%
  rename(percentage_party = fdp_RA)

linke_df <- dataframe_election %>%
  select(linke_RA, end_date) %>%
  rename(percentage_party = linke_RA)

grune_df <- dataframe_election %>%
  select(grune_RA, end_date) %>%
  rename(percentage_party = grune_RA)

```

``` {r, plot_of_graph, warnings = FALSE, message = FALSE}
#Plot data on graph
ggplot(union_df, aes(x=end_date, y=percentage_party)) +
  geom_point(alpha = 0.3) +
  geom_smooth(colour='black') + 
  geom_point(data=spd_df,colour='firebrick3',alpha = 0.3) +
  geom_smooth(data=spd_df,colour='firebrick3') +
  geom_point(data=af_d_df,colour='deepskyblue3',alpha = 0.3) +
  geom_smooth(data=af_d_df,colour='deepskyblue3') +
  geom_point(data=fdp_df,colour='yellow2',alpha = 0.3) +
  geom_smooth(data=fdp_df,colour='yellow2') +
  geom_point(data=linke_df,colour='violetred3',alpha = 0.3) +
  geom_smooth(data=linke_df,colour='violetred3') +
  geom_point(data=grune_df,colour='chartreuse3',alpha = 0.3) +
  geom_smooth(data=grune_df,colour='chartreuse3') + #Add data points for each political party and then a best fit line
  theme_bw()+
  labs(title = "Opinion Polling for the 2021 German Federal Election",
       x = "Date", 
       y = "Predicted Percentage of Votes",
       caption = "Source: https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election 
       Legend: Union = Black, SPD = Red, AFD = Blue, FDP = Yellow, Linke = Purple, Gr√ºne = Green",
       ) +
  NULL

```
This graph depicts the predicted percentage of votes for each party in the German federal election over the past year, based on opinion polls that have been collected.


# Details

-   Who did you collaborate with: Lindsey Zastawny, Danqing Huang, Aneta Gajdzinska, Shuyi Guo, Gautam Sreekumar, Lukas Kugler
-   Approximately how much time did you spend on this problem set: 8 hours
-   What, if anything, gave you the most trouble: Trying not to cry
